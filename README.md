# Text-Summariser-NLP

The project aims to develop a text summarizer using natural language processing techniques with an extractive approach. The summarizer will analyze a given text and identify the most important sentences based on their relevance to the overall content. The system will use techniques such as sentence parsing, keyword extraction, and scoring algorithms to rank and select the most informative sentences. The resulting summary will provide a concise and coherent representation of the original text, enabling users to quickly grasp the main points without having to read the entire document. This project has the potential to assist in various fields such as news agencies, researchers, and businesses to quickly extract the most significant information from large texts.

## Objectives
1. Condensing large amount of text into a shorter version that retains most important information
2. Saving time and effort
3. Improving information retrieval
4. Improving language processing

## Introduction
Text summarization using NLP involves the automatic generation of a shortened version of a given text document, while still retaining its key information and meaning. This project is based on the application of Natural Language Processing (NLP) techniques, which involves the analysis of the language and structure of the text, to extract important sentences and phrases and form a summary.The project involves several steps, such as preprocessing the text data, identifying important sentences and phrases, and generating the summary.

To achieve this, various NLP techniques can be employed, such as text segmentation, part-of-speech tagging, named entity recognition, word embedding, and machine learning algorithms such as clustering and classification.
## Architecture
//Diagrams with canva showing various processes of NLP used eg: Preprocessing->Stemming->Lemmatisation,
  Mathematics involved ie maybe wordvectors used and the theory behind them.
  Diagrams to be explained in depth//
  

## Dataset used
The dataset used is an article of around 5000 words, obtained from Wikipedia, in .txt format that is uploaded in the session storage of the colab notebook for it to be read by the model.

## Performance and Results
The model summarises a dataset containing around 5000 words to produce an accurate summary of 340 words.

The current model that we have presented is based on word and sentence frequency along with considering the presence of stopwords. It could further be improvised by incorporating the concept of word embeddings, where the meaning of the word is given better importance.

## References
https://en.wikipedia.org/wiki/Management

https://www.analyticsvidhya.com/blog/2022/02/a-flask-web-app-for-automatic-text-summarization-using-sbert/

https://pub.towardsai.net/a-step-by-step-approach-to-building-a-text-summarization-webapp-in-python-from-scratch-5d6754306f3b

## Project Mentors

## Project Mentees
  


